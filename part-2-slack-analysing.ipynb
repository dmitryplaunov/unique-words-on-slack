{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unique words of Slack users\n",
    "\n",
    "Part II. A Python script for estimating the most distinguishing words of Slack users.<br>\n",
    "Part I consisted of JavaScript code to scrape messages and save them in TXT files.<br>\n",
    "Full description on [GitHub](https://github.com/dmitryplaunov/unique-words-on-slack). Copyright (c) 2020 Dmitry Plaunov; Licensed MIT.\n",
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os import listdir\n",
    "import re\n",
    "\n",
    "# a path to the folder with TXT files that contain scraped messages ...\n",
    "# ... from each Slack user that you want to analyse\n",
    "scraped_messages_folder = 'team/'\n",
    "\n",
    "# creating an empty list that will be populated with names of TXT files ...\n",
    "# ... that will be used as identifiers (names of Slack users)\n",
    "all_users = []\n",
    "\n",
    "# creating an empty dataframe that will be populated with counts of words ...\n",
    "# ... from each Slack user\n",
    "all_together = pd.DataFrame()\n",
    "\n",
    "# opening each file from the folder\n",
    "for file in listdir(scraped_messages_folder):\n",
    "    f = open(scraped_messages_folder + file, 'r', encoding='utf8')\n",
    "    all_words = f.read()\n",
    "    f.close()\n",
    "    \n",
    "    # getting the user's name from the name of the file and adding it to the list\n",
    "    user_name = file.rsplit('.',1)[0]\n",
    "    all_users.append(user_name)\n",
    "    \n",
    "    # ________\n",
    "    #\n",
    "    # Part I. Data cleaning (removing links, punctuation, numbers, stopwords, etc.)\n",
    "    # ________\n",
    "    #\n",
    "    \n",
    "    # removing links\n",
    "    all_words = re.sub('http[:\\.\\?\\&\\%\\@\\+_#=\\-/A-Za-z0-9]*', '', all_words)\n",
    "\n",
    "    # removing special characters and punctuation\n",
    "    all_words = re.sub('[\\?\\!\\.,:;”“«»’‘•><—=+\"()/\\&\\*\\-\\{\\}]', '', all_words)\n",
    "    all_words = re.sub(\"[']\", '', all_words)\n",
    "\n",
    "    # unifying words by making them all lowercase\n",
    "    all_words = all_words.lower()\n",
    "    \n",
    "    # removing messages that say that the user just uploaded a file\n",
    "    all_words.replace('uploaded a file', '')\n",
    "    \n",
    "    # removing names of images; other file extensions don't appear in text messages (as of April 2020)\n",
    "    all_words = re.sub('[A-Za-z]*\\.jpg', '', all_words)\n",
    "    all_words = re.sub('[A-Za-z]*\\.png', '', all_words)\n",
    "    all_words = re.sub('[A-Za-z]*\\.gif', '', all_words)\n",
    "    \n",
    "    # removing numbers\n",
    "    all_words = re.sub('[0-9]', '', all_words)\n",
    "\n",
    "    # removing mentions and other words that start with @, except '@here'\n",
    "    all_words = re.sub('(?!@here)@[a-zA-Z]*', '', all_words)\n",
    "\n",
    "    # defining word combinations (up to 3) that should be treated as 1 word in statistics\n",
    "    combined_words = ['ab test',\n",
    "                      'ab testing',\n",
    "                      'google analytics',\n",
    "                      'dynamic yield',\n",
    "                      'product analytics',\n",
    "                      'is down',\n",
    "                      'stand up',\n",
    "                      'feel better',\n",
    "                      'product owner',\n",
    "                      'materialized view']    \n",
    "    \n",
    "    for combined_word in combined_words:\n",
    "        all_words = all_words.replace(combined_word, combined_word.replace(' ','').replace(' ',''))\n",
    "    \n",
    "    # creating a set of stop words, the words that will have little value in analysing slack users, ...\n",
    "    # ... so they should be removed from the statistics\n",
    "    stopwords = {\"a\",\"about\",\"above\",\"after\",\"again\",\"against\",\"ain\",\"all\",\"also\",\"am\",\"an\",\"and\",\"any\",\"are\",\n",
    "                 \"aren\",\"arent\",\"as\",\"at\",\"be\",\"because\",\"been\",\"before\",\"being\",\"below\",\"between\",\n",
    "                 \"both\",\"but\",\"by\",\"can\",\"couldn\",\"couldnt\",\"did\",\"didn\",\"didnt\",\"do\",\"does\",\"doesn\",\n",
    "                 \"doesnt\",\"doing\",\"don\",\"dont\",\"down\",\"during\",\"each\",\"few\",\"for\",\"from\",\"further\",\n",
    "                 \"had\",\"hadn\",\"hadnt\",\"has\",\"hasn\",\"hasnt\",\"have\",\"haven\",\"havent\",\"having\",\"he\",\"her\",\n",
    "                 \"here\",\"hers\",\"herself\",\"him\",\"himself\",\"his\",\"how\",\"i\",\"im\",\"ill\",\"ive\",\"if\",\"in\",\n",
    "                 \"into\",\"is\",\"isn\",\"isnt\",\"it\",\"its\",\"its\",\"itself\",\"just\",\"me\",\"mightn\",\"mightnt\",\n",
    "                 \"more\",\"most\",\"mustn\",\"mustnt\",\"my\",\"myself\",\"needn\",\"neednt\",\"no\",\"nor\",\"not\",\"now\",\n",
    "                 \"o\",\"of\",\"off\",\"on\",\"one\",\"once\",\"only\",\"or\",\"other\",\"our\",\"ours\",\"ourselves\",\"out\",\"over\",\n",
    "                 \"own\",\"re\",\"s\",\"same\",\"shan\",\"shant\",\"she\",\"shes\",\"should\",\"shouldve\",\"shouldn\",\n",
    "                 \"shouldnt\",\"since\",\"so\",\"some\",\"still\",\"such\",\"than\",\"that\",\"thatll\",\"the\",\"their\",\"theirs\",\"them\",\n",
    "                 \"themselves\",\"then\",\"there\",\"these\",\"they\",\"this\",\"those\",\"through\",\"to\",\"too\",\"under\",\n",
    "                 \"until\",\"up\",\"ve\",\"very\",\"was\",\"wasn\",\"wasnt\",\"we\",\"were\",\"weren\",\"werent\",\"what\",\n",
    "                 \"when\",\"where\",\"which\",\"while\",\"who\",\"whom\",\"why\",\"will\",\"with\",\"won\",\"wont\",\"would\",\"wouldn\",\n",
    "                 \"wouldnt\",\"you\",\"youd\",\"youll\",\"youre\",\"youve\",\"your\",\"yours\",\"yourself\",\"yourselves\"}\n",
    "    \n",
    "    # creating a list of all words\n",
    "    words = all_words.split()\n",
    "    \n",
    "    # creating a variable that will be appended with words that are not stop words\n",
    "    updated_all_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        if word not in stopwords:\n",
    "            updated_all_words.append(word)\n",
    "    \n",
    "    # ________\n",
    "    #\n",
    "    # Part II. Counting words for each Slack user\n",
    "    # ________\n",
    "    #\n",
    "    \n",
    "    # creating an empty dictionary that will be populated with counts of words\n",
    "    counts = dict()\n",
    "    \n",
    "    # counting the number of each word\n",
    "    for word in updated_all_words:\n",
    "        if word in counts:\n",
    "            counts[word] += 1\n",
    "        else:\n",
    "            counts[word] = 1\n",
    "\n",
    "    # transforming a dictionary into a dataframe\n",
    "    word_items = counts.items()\n",
    "    word_list = list(word_items)\n",
    "    word_df = pd.DataFrame(word_list)\n",
    "\n",
    "    # defining columns in the dataframe\n",
    "    word_df.columns = ['word', user_name]\n",
    "    \n",
    "    # sorting columns by user names\n",
    "    word_df = word_df.sort_values(user_name, ascending=False)\n",
    "    \n",
    "    # using 'word' column as index\n",
    "    word_df.index = word_df.word\n",
    "    \n",
    "    # deleting the 'word' column (not index)\n",
    "    word_df = word_df.drop('word', axis='columns')\n",
    "\n",
    "    # concatenating the dataframe horizontally (axis=1) with the counts of words for each user\n",
    "    all_together = pd.concat([all_together, word_df], axis=1)\n",
    "    \n",
    "    print('Words from ', user_name, ' are analysed', sep='')\n",
    "    \n",
    "\n",
    "# ________\n",
    "#\n",
    "# Part III. Using the formula to get the most unique words for each user\n",
    "# ________\n",
    "#    \n",
    "    \n",
    "# creating a 'Total' row and column\n",
    "all_together.loc[\"Total\"] = all_together.sum(axis=0)\n",
    "all_together[\"Total\"] = all_together.sum(axis=1)\n",
    "\n",
    "for user in all_users:\n",
    "    column_name = user+'_est_uniq'\n",
    "    all_together[column_name] = all_together[user]**2/all_together[\"Total\"]*all_together.loc[\"Total\", \"Total\"]/all_together.loc[\"Total\", user]\n",
    "\n",
    "print('Done!')\n",
    "print('')\n",
    "    \n",
    "# exporting a CSV file with the most unique words estimation\n",
    "all_together.to_csv('unique_words.csv', encoding='utf-8-sig')\n",
    "\n",
    "print(\"The data is exported to 'unique_words.csv'.\")\n",
    "print(\"The output consists of word counts and estimated word uniqueness.\")\n",
    "print(\"Use the words that scored the highest for each user and also the highest among other users to identify the most unique words.\")\n",
    "#print(all_together)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
